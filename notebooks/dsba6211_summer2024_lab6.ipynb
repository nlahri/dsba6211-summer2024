{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nlahri/dsba6211-summer2024/blob/main/notebooks/dsba6211_summer2024_lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HIuaawzR8RR"
      },
      "source": [
        "# Getting Started with Prompt Engineering\n",
        "\n",
        "This notebook contains examples and exercises to learning about prompt engineering. It was originally created by DAIR.AI | Elvis Saravia with modifications.\n",
        "\n",
        "We will be using the [OpenAI APIs](https://platform.openai.com/) for all examples. I am using the default settings `temperature=0.7` and `top-p=1`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jego_SZUR8RR"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0do4hkhR8RS"
      },
      "source": [
        "## 1. Prompt Engineering Basics: OpenAI API and configurations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOMd4TbUR8RS"
      },
      "source": [
        "Below we are loading the necessary libraries, utilities, and configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R1KWAb3_R8RS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "!pip install --upgrade openai==1.35.1\n",
        "!pip install --upgrade langchain==0.2.5\n",
        "!pip install --upgrade langchain-openai==0.1.8\n",
        "!pip install langchain-community==0.2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yANHPO1MR8RS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import IPython\n",
        "from langchain.llms import OpenAI\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YhuFzr-R8RS"
      },
      "source": [
        "Load environment variables. Since I'm running this in a Colab notebook, I'm using `userdata.get()`. Just make sure to add your API Key into your Colab.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wEevNCOf80GTHwptPTB4g.png)\n",
        "\n",
        "Alternatively, you can use `python-dotenv` with a `.env` file with your `OPENAI_API_KEY` then load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Qs7ie0kR8RS"
      },
      "outputs": [],
      "source": [
        "# API configuration\n",
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# for LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of Endpoints\n",
        "\n",
        "OpenAI (and other LLM) API's typically have two types API endpoints: completion and chat.\n",
        "\n",
        "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf3b37b-ad35-43b3-8cac-607a01473ba8_2719x508.png)\n",
        "\n",
        "Source: [Generally Intelligent SubStack](https://generallyintelligent.substack.com/p/chat-vs-completion-endpoints)\n",
        "\n",
        "Originally, the completions endpoint was the first endpoint. However, OpenAI has [announced](https://community.openai.com/t/completion-models-are-now-considered-legacy/656302) it is deprecating that endpoint.\n",
        "\n",
        "`/completions` endpoint provides the completion for a single prompt and takes a single string as an input, whereas the `/chat/completions` provides the responses for a given dialog and requires the input in a specific format corresponding to the message history. Instead of taking a `prompt`, Chat models take a list of `messages` as input and return a model-generated message as output.\n",
        "\n",
        "\n",
        "\n",
        "| MODEL FAMILIES               | EXAMPLES                                         | API ENDPOINT                               |\n",
        "|------------------------------|--------------------------------------------------|--------------------------------------------|\n",
        "| Newer models (2023â€“)         | gpt-4, gpt-4-turbo-preview, gpt-3.5-turbo        | https://api.openai.com/v1/chat/completions |\n",
        "| Updated legacy models (2023) | gpt-3.5-turbo-instruct, babbage-002, davinci-002 | https://api.openai.com/v1/completions      |\n",
        "\n",
        "\n",
        "Although the chat format is designed to make multi-turn conversations easy, itâ€™s just as useful for single-turn tasks without any conversation.\n",
        "\n",
        "We're going to the use the Chat `openai.chat.completions` endpoint.\n",
        "\n",
        "For more details, check out [OpenAI's docs](https://platform.openai.com/docs/guides/text-generation/chat-completions-vs-completions)."
      ],
      "metadata": {
        "id": "nAbquZOc9hjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "#response"
      ],
      "metadata": {
        "id": "fKY-ePiJC3y6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.choices[0].message.content"
      ],
      "metadata": {
        "id": "dXsHcbAaQdiR",
        "outputId": "63634104-d296-4015-e849-222944c2f2cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, what we're most interested in is the `choices[0].message.content`:"
      ],
      "metadata": {
        "id": "jcUqxHrqDVeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TrAlcD1KEJ0X",
        "outputId": "fdc3bdd2-4a7f-41d0-9cae-85db7350488c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make things a bit easier to modify parameters, we can generalize the calls as a function, including parameters and messages."
      ],
      "metadata": {
        "id": "_JHq92uCDF38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "S9HGq2JQR8RS"
      },
      "outputs": [],
      "source": [
        "def get_completion(params, messages):\n",
        "    \"\"\" GET completion from openai api\"\"\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model = params['model'],\n",
        "        messages = messages,\n",
        "        temperature = params['temperature'],\n",
        "        max_tokens = params['max_tokens'],\n",
        "        top_p = params['top_p'],\n",
        "        frequency_penalty = params['frequency_penalty'],\n",
        "        presence_penalty = params['presence_penalty'],\n",
        "    )\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a variety of different parameters that are common with LLM's. Since LLM's are really just word predictors (auto-complete, distributions over vocabulary), they require [different sampling methods](https://huyenchip.com/2024/01/16/sampling.html) to get the next word.\n",
        "\n",
        "| Parameter          | Description                                                                                                                                                   |\n",
        "|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| `model`            | Specifies the model to be used for generating responses. Different models may have different capabilities, size, and performance characteristics.             |\n",
        "\n",
        "Here's [an outline](https://www.promptingguide.ai/introduction/settings) of different common LLM parameters.\n"
      ],
      "metadata": {
        "id": "rqY79t7q8SFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ24btuoHvu9",
        "outputId": "f574f6fb-a654-41ae-b42d-c72ac4fd5a02",
        "collapsed": true
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: openai [-h] [-v] [-b API_BASE] [-k API_KEY] [-p PROXY [PROXY ...]] [-o ORGANIZATION]\n",
            "              [-t {openai,azure}] [--api-version API_VERSION] [--azure-endpoint AZURE_ENDPOINT]\n",
            "              [--azure-ad-token AZURE_AD_TOKEN] [-V]\n",
            "              {api,tools,migrate,grit} ...\n",
            "\n",
            "positional arguments:\n",
            "  {api,tools,migrate,grit}\n",
            "    api                 Direct API calls\n",
            "    tools               Client side tools for convenience\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -v, --verbose         Set verbosity.\n",
            "  -b API_BASE, --api-base API_BASE\n",
            "                        What API base url to use.\n",
            "  -k API_KEY, --api-key API_KEY\n",
            "                        What API key to use.\n",
            "  -p PROXY [PROXY ...], --proxy PROXY [PROXY ...]\n",
            "                        What proxy to use.\n",
            "  -o ORGANIZATION, --organization ORGANIZATION\n",
            "                        Which organization to run as (will use your default organization if not\n",
            "                        specified)\n",
            "  -t {openai,azure}, --api-type {openai,azure}\n",
            "                        The backend API to call, must be `openai` or `azure`\n",
            "  --api-version API_VERSION\n",
            "                        The Azure API version, e.g. 'https://learn.microsoft.com/en-us/azure/ai-\n",
            "                        services/openai/reference#rest-api-versioning'\n",
            "  --azure-endpoint AZURE_ENDPOINT\n",
            "                        The Azure endpoint, e.g. 'https://endpoint.openai.azure.com'\n",
            "  --azure-ad-token AZURE_AD_TOKEN\n",
            "                        A token from Azure Active Directory, https://www.microsoft.com/en-\n",
            "                        us/security/business/identity-access/microsoft-entra-id\n",
            "  -V, --version         show program's version number and exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_open_params(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "):\n",
        "    \"\"\" set openai parameters\"\"\"\n",
        "\n",
        "    openai_params = {}\n",
        "\n",
        "    openai_params['model'] = model\n",
        "    openai_params['temperature'] = temperature\n",
        "    openai_params['max_tokens'] = max_tokens\n",
        "    openai_params['top_p'] = top_p\n",
        "    openai_params['frequency_penalty'] = frequency_penalty\n",
        "    openai_params['presence_penalty'] = presence_penalty\n",
        "    return openai_params"
      ],
      "metadata": {
        "id": "6EWfKXQfBgm8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8Ljy2AtbR8RT"
      },
      "outputs": [],
      "source": [
        "# basic example\n",
        "\n",
        "params = set_open_params(temperature = 0.7)\n",
        "\n",
        "prompt = \"Write a haiku about a beagle.\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "cijXEzzVR8RT",
        "outputId": "5dd7ca6a-2780-41df-fd3f-68324d27f13e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Playful beagle pup\nSniffing out adventures near\nTail wagging with joy"
          },
          "metadata": {}
        }
      ],
      "source": [
        "IPython.display.display(IPython.display.Markdown(response.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature"
      ],
      "metadata": {
        "id": "nwmhE4TvI__c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36jqmKk7R8RT"
      },
      "source": [
        "\n",
        "| Parameter          | Description                                                                                                                                                   |\n",
        "|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| `temperature`      | Controls randomness in the response generation. A higher temperature results in more random responses, while a lower temperature produces more deterministic responses. |\n",
        "\n",
        "Try to modify the temperature from 0 to 1.\n",
        "\n",
        "> In terms of application, you might want to use a lower temperature value for tasks like fact-based QA to encourage more factual and concise responses. For poem generation or other creative tasks, it might be beneficial to increase the temperature value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "2OxiV-uQR8RT",
        "outputId": "baef84f4-25c1-460f-85fb-0fdad003f9eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with temperature=0:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Curious beagle nose\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sniffing out adventures near\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Tail wags in delight"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with temperature=0.2:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Curious beagle nose\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sniffing out adventures near\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Tail wags in delight"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with temperature=0.4:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Curious beagle nose\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sniffs out every scent with glee\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Tail wags in delight"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with temperature=0.6:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Playful floppy ears\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sniffing out scents in the air\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Beagle on the trail"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with temperature=0.8:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Curious beagle nose\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sniffing all around the yard\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Tail wagging with joy"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with temperature=1:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Curious beagle nose\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sniffing scents along the trail\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Tail wagging happily"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def split_on_capital(text):\n",
        "    return re.findall(r'[A-Z][^A-Z]*', text)\n",
        "\n",
        "# Experiment with different temperature values\n",
        "for temp in [0, 0.2, 0.4, 0.6, 0.8, 1]:\n",
        "    params = set_open_params(temperature=temp)\n",
        "    response = get_completion(params, messages)\n",
        "    lines = split_on_capital(response.choices[0].message.content.strip())\n",
        "    print(f\"Response with temperature={temp}:\\n\")\n",
        "    for line in lines:\n",
        "        IPython.display.display(IPython.display.Markdown(line))\n",
        "    print(\"-------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[View how I used ChatGPT to iterate on this code](https://chat.openai.com/share/6211107a-d868-491b-8a74-ea4debb7a760)"
      ],
      "metadata": {
        "id": "46-IqrqgNFdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### ðŸ—’ Info: Is it possible to make LLM's deterministic and reproducibile?\n",
        "\n",
        "> It's possible by setting a seed and setting temperature equal to 0. But as mentioned in [OpenAI's docs](https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter), \"it's important to note that while the seed ensures consistency, it does not guarantee the quality of the output.\""
      ],
      "metadata": {
        "id": "TkSDDqo9iQl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top P\n",
        "\n",
        "Used in nucleus sampling, it defines the probability mass to consider for token generation. A smaller `top_p` leads to more focused sampling.\n",
        "\n",
        "* `top_p` computes the cumulative probability distribution, and cut off as soon as that distribution exceeds the value of `top_p`. **For example, a `top_p` of 0.3 means that only the tokens comprising the top 30% probability mass are considered.**\n",
        "\n",
        "* `top_p` shrinks or grows the \"pool\" of available tokens to choose from, the domain to select over. 1=big pool, 0=small pool. Within that pool, each token has a probability of coming next.\n",
        "\n",
        "Added in a `jinja2` template too (this is optional, but commonly used)."
      ],
      "metadata": {
        "id": "PMEu7ah-R_pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install textstat"
      ],
      "metadata": {
        "id": "dG82BktkYQfu"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from textstat import flesch_kincaid_grade\n",
        "from jinja2 import Template\n",
        "\n",
        "# Define the prompt using a Jinja template\n",
        "template = Template(\"\"\"\n",
        "{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"{{ prompt }}\"\n",
        "}\n",
        "\"\"\")\n",
        "prompt = \"Generate a unique and creative story idea involving time travel.\"\n",
        "\n",
        "# Function to count unique words\n",
        "def count_unique_words(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return len(set(words))\n",
        "\n",
        "# Generate the message using the template\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "for top_p in [0.1, 0.5, 0.95]:\n",
        "    params = set_open_params(top_p=top_p)\n",
        "    response = get_completion(params, messages)\n",
        "    text = response.choices[0].message.content\n",
        "    unique_word_count = count_unique_words(text)\n",
        "    fk_score = flesch_kincaid_grade(text)\n",
        "    print(f\"Response with top_p={top_p}:\\n\")\n",
        "    print(f\"Unique word count: {unique_word_count}\")\n",
        "    print(f\"Flesch-Kincaid Grade Level: {fk_score}\")\n",
        "    IPython.display.display(IPython.display.Markdown(text))\n",
        "    print(\"-------------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "id": "x-S0B2IHR0Th",
        "outputId": "8d492027-2150-483b-9602-b7d860e8a234"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with top_p=0.1:\n",
            "\n",
            "Unique word count: 132\n",
            "Flesch-Kincaid Grade Level: 10.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In the year 3025, time travel has become a common form of entertainment for the wealthy elite. The Time Travel Corporation offers exclusive trips to different eras in history, allowing clients to witness major events firsthand.\n\nOne day, a young woman named Eliza wins a contest to travel back to the year 1920 and attend a lavish party hosted by the infamous gangster Al Capone. Excited for the adventure, Eliza steps into the time machine and is transported back in time.\n\nHowever, when she arrives in 1920, Eliza quickly realizes that something has gone wrong. Instead of being a mere observer, she finds herself in the middle of a dangerous plot to assassinate Al Capone. Unsure of how to navigate this new reality, Eliza must use her wits and resourcefulness to survive in a time period vastly different from her own.\n\nAs Eliza delves deeper into the world of 1920s Chicago, she uncovers dark secrets and hidden agendas that threaten to alter the course of history. With the help of a charming but enigmatic detective, Eliza must race against time to unravel the mystery and prevent a catastrophic event from changing the future forever.\n\nThrough her journey, Eliza learns valuable lessons about the consequences of meddling with the past"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with top_p=0.5:\n",
            "\n",
            "Unique word count: 128\n",
            "Flesch-Kincaid Grade Level: 8.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In the year 3021, time travel has become a common form of entertainment for the wealthy elite. The Time Travel Corporation offers exclusive trips to different time periods for those willing to pay the hefty price. \n\nOne day, a young woman named Lily wins a contest to travel back in time to the year 2021. Excited for the opportunity to experience life in the past, she eagerly steps into the time machine and is transported back in time.\n\nHowever, when Lily arrives in 2021, she quickly realizes that something is not right. The world she has traveled to is not the same as the one she had read about in history books. The technology is more advanced, the cities are cleaner, and the people seem to be living in harmony with nature.\n\nAs Lily explores this new world, she discovers that a group of rogue scientists had discovered the secret to time travel in the early 2000s and had been using it to alter the course of history. They had been traveling back in time to make small changes that would have a ripple effect on the future, creating a utopian society in the present.\n\nDetermined to uncover the truth behind this mysterious society, Lily teams up with a group of rebels who are fighting against the oppressive regime. Together, they must"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with top_p=0.95:\n",
            "\n",
            "Unique word count: 120\n",
            "Flesch-Kincaid Grade Level: 10.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In the year 3021, time travel has been perfected and is used for historical research and entertainment purposes. The protagonist, a young archaeologist named Ava, discovers a mysterious artifact during a dig in ancient Egypt. When she accidentally activates it, she is transported back in time to the year 1345 BC.\n\nAva finds herself in the midst of political turmoil and must navigate the dangerous world of ancient Egypt to find a way back to her own time. Along the way, she befriends a group of rebels who are fighting against the tyrannical ruler of Egypt. As Ava helps them in their quest for freedom, she begins to uncover the true power of the artifact she found and its connection to her own past.\n\nAs Ava delves deeper into the mysteries of ancient Egypt, she discovers that her actions in the past have consequences that ripple through time. She must make difficult choices that will not only determine the fate of ancient Egypt, but also her own future.\n\nThrough her journey, Ava learns the importance of preserving history while also understanding the impact of her actions on the timeline. Ultimately, she must decide whether to return to her own time or stay in ancient Egypt to fulfill her destiny and change the course of history forever."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **If you are looking for exact and factual answers keep this low. If you are looking for more diverse responses, increase to a higher value.** If you use Top P it means that only the tokens comprising the top_p probability mass are considered for responses, so a low top_p value selects the most confident responses. This means that a high top_p value will enable the model to look at more possible words, including less likely ones, leading to more diverse outputs. **The general recommendation is to alter temperature or Top P but not both.**"
      ],
      "metadata": {
        "id": "NBhg8_glafVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What day is it?\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "for top_p in [0.5, 0.8, 0.95]:\n",
        "    params = set_open_params(top_p=top_p)\n",
        "    response = get_completion(params, messages)\n",
        "    print(f\"Response with top_p={top_p}:\\n\")\n",
        "    IPython.display.display(IPython.display.Markdown(response.choices[0].message.content))\n",
        "    print(\"-------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "zZ-en7l3TRLN",
        "outputId": "a3f0336c-cc4f-422c-eb07-70c3df65bf66"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with top_p=0.5:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I am an AI digital assistant and do not have the ability to know the current day. Please check your calendar or device for the current date."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with top_p=0.8:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I'm sorry, I am an AI assistant and I do not have the ability to know the current date."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with top_p=0.95:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I am a language model AI and I am not able to provide real-time information. Please check your device for the current date."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max Tokens\n",
        "\n",
        "Defines the maximum length of the generated response measured in tokens (words or pieces of words). It helps in controlling the verbosity of the response.\n",
        "\n",
        "* **Prompt**: \"Explain how photosynthesis works in plants.\"\n",
        "* **Task**: Run the API three times with `max_tokens` set to 50, 100, and 200, respectively."
      ],
      "metadata": {
        "id": "0p8tzYliJCtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with different max_tokens values\n",
        "prompt = \"Explain how photosynthesis works in plants.\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "for max_tokens in [50, 100, 200]:\n",
        "    params = set_open_params(max_tokens=max_tokens)\n",
        "    response = get_completion(params, messages)\n",
        "    print(f\"Response with max_tokens={max_tokens}:\\n\")\n",
        "    IPython.display.display(IPython.display.Markdown(response.choices[0].message.content))\n",
        "    print(\"-------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "16W-v3mrI-eT",
        "outputId": "78e7cb0b-a6e9-49b8-c86e-6592f4d6b91c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with max_tokens=50:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into oxygen and glucose (a type of sugar). This process occurs in the chloroplasts of plant cells.\n\nDuring photosynthesis, sunlight is"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with max_tokens=100:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, carbon dioxide, and water into glucose (a type of sugar) and oxygen. This process occurs in the chloroplasts of plant cells.\n\nDuring photosynthesis, sunlight is absorbed by chlorophyll, a green pigment found in the chloroplasts. The energy from the sunlight is used to split water molecules into oxygen and hydrogen ions. The oxygen is released as a byproduct, while the hydrogen ions are used to"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with max_tokens=200:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, carbon dioxide, and water into glucose (sugar) and oxygen. This process occurs in the chloroplasts of plant cells, specifically in the thylakoid membranes.\n\nThe process of photosynthesis can be broken down into two main stages: the light-dependent reactions and the light-independent reactions (also known as the Calvin cycle).\n\nDuring the light-dependent reactions, light energy is absorbed by chlorophyll molecules in the chloroplasts. This energy is used to split water molecules into oxygen, protons, and electrons. The oxygen is released as a byproduct, while the protons and electrons are used to create ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate), which are both energy-carrying molecules.\n\nIn the light-independent reactions (Calvin cycle), the ATP and NADPH produced in the light-dependent reactions are used to convert"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiT0Cw4ZR8RT"
      },
      "source": [
        "### 1.1 Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "XP7CSLfTR8RT",
        "outputId": "66343e67-932b-4fc7-9b25-2b488536fe72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Antibiotics are medications used to treat bacterial infections by either killing or preventing the bacteria from reproducing, but they are not effective against viral infections and should be used appropriately to avoid antibiotic resistance."
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "params = set_open_params(temperature=0.7)\n",
        "\n",
        "prompt = \"\"\"Explain the below in one sentence: Antibiotics are a type of medication \\\n",
        "          used to treat bacterial infections. They work by either killing the bacteria \\\n",
        "          or preventing them from reproducing, allowing the body's immune system to \\\n",
        "          fight off the infection. Antibiotics are usually taken orally in the form \\\n",
        "          of pills, capsules, or liquid solutions, or sometimes administered intravenously. \\\n",
        "          They are not effective against viral infections, and using them inappropriately \\\n",
        "          can lead to antibiotic resistance.\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzOToTAxR8RT"
      },
      "source": [
        "> **Exercise**: Instruct the model to explain the paragraph in one sentence like \"I am 5\". Do you see any differences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pExMtUo-R8RT"
      },
      "source": [
        "### 1.2 Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "X8lYXWErR8RT",
        "outputId": "1f836320-7589-4225-e554-d8af51f89848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Mice"
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. \\\n",
        " Keep the answer short and concise. \\\n",
        " Respond 'Unsure about answer' if not sure about the answer. \\\n",
        " Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. \\\n",
        " There, scientists generated an early version of the antibody, dubbed OKT3. \\\n",
        " Originally sourced from mice, the molecule was able to bind to the surface of T cells \\\n",
        " and limit their cell-killing potential. In 1986, it was approved to help prevent organ \\\n",
        " rejection after kidney transplants, making it the first therapeutic antibody allowed \\\n",
        " for human use. \\\n",
        " Question: What was OKT3 originally sourced from? \\\n",
        " Answer:\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLg9Z2i4R8RT"
      },
      "source": [
        "Context obtained from here: https://www.nature.com/articles/d41586-023-00400-x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrtEMAPsR8RU"
      },
      "source": [
        "> **Exercise**: Edit prompt and get the model to respond that it isn't sure about the answer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = set_open_params(temperature=0.6)\n",
        "\n",
        "prompt = \"\"\"Answer the question based on the context below. \\\n",
        " Keep the answer short and concise. \\\n",
        " Respond 'Unsure about answer' if not sure about the answer. \\\n",
        " Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. \\\n",
        " There, scientists generated an early version of the antibody, dubbed OKT3. \\\n",
        " Originally sourced from mice, the molecule was able to bind to the surface of T cells \\\n",
        " and limit their cell-killing potential. In 1986, it was approved to help prevent organ \\\n",
        " rejection after kidney transplants, making it the first therapeutic antibody allowed \\\n",
        " for human use. \\\n",
        " Question: What is replaced by OKT3 ?  \\\n",
        " Answer:\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)\n",
        "#Answer only when you are 100 percent sure. Else say you are not sure."
      ],
      "metadata": {
        "id": "oHQmdyQjR-FW",
        "outputId": "44ab64cd-d447-42ed-aa80-045b7b9bc684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Unsure about answer"
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dso3RN7sR8RU"
      },
      "source": [
        "### 1.3 Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "wjw0n9rFR8RU",
        "outputId": "33d4571c-39e1-4af7-8d99-bb8919304a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Positive"
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the text into neutral, negative or positive. \\\n",
        "                                                  \\\n",
        "            Text: I think the food was amazing!   \\\n",
        "                                                  \\\n",
        "            Sentiment:\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ti6BNfnR8RU"
      },
      "source": [
        "> **Exercise**: Modify the prompt to instruct the model to provide an explanation to the answer selected."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Classify the text into neutral, negative or positive and Explain with a short reasoning \\\n",
        "                                                  \\\n",
        "            Text: I think the food was amazing!   \\\n",
        "                                                  \\\n",
        "            Sentiment:\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "qsJfCshsUNDX",
        "outputId": "d8c53909-6f56-4187-a154-0e3842377eb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Positive\n\nReasoning: The use of the word \"amazing\" indicates a positive sentiment towards the food. The speaker is expressing their enjoyment and satisfaction with the food."
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apKYpWJMR8RU"
      },
      "source": [
        "### 1.4 Role Playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "5c-L0twDR8RU",
        "outputId": "c2a4614a-8d68-455f-fa11-99d4f400f48b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Of course. Black holes are formed when massive stars collapse under their own gravity after exhausting their nuclear fuel. This collapse causes the star's core to condense into an extremely dense region with a gravitational pull so strong that not even light can escape. This region is known as a black hole."
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI research assistant. \\\n",
        "The assistant tone is technical and scientific. \\\n",
        "                                                \\\n",
        "Human: Hello, who are you? \\\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today? \\\n",
        "Human: Can you tell me about the creation of blackholes? \\\n",
        "AI:\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GTLlgAaR8RU"
      },
      "source": [
        "> **Exercise**: Modify the prompt to instruct the model to keep AI responses concise and short."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI research assistant. \\\n",
        "The assistant tone is technical and scientific. \\\n",
        "                                                \\\n",
        "Human: Hello, who are you? \\\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today? \\\n",
        "Human: Can you tell me about the creation of blackholes? Be concise and crisp \\\n",
        "AI:\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "#params = set_open_params(max_tokens=100)\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "Cu4RoNmHUjxE",
        "outputId": "9f984cce-ccbd-4330-97c9-7771625ea84e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Black holes are formed when a massive star collapses under its own gravity, creating a singularity with infinite density and a gravitational pull so strong that not even light can escape. This process is known as gravitational collapse."
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH2Xt1dTR8RU"
      },
      "source": [
        "### 1.5 Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "cQpcLZ3cR8RU",
        "outputId": "babafe61-f97e-471f-e5c6-fe754431bb59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure! Here is a simple Python API that functions as a virtual plant care assistant. This API will provide basic functions such as watering the plant, checking the soil moisture level, and providing recommendations for plant care.\n\n```python\nclass PlantCareAssistant:\n    def __init__(self, plant_name, soil_moisture_level):\n        self.plant_name = plant_name\n        self.soil_moisture_level = soil_moisture_level\n\n    def water_plant(self):\n        self.so"
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "prompt = \"\"\"Develop a small Python API that functions as a virtual plant care assistant.\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfjREa5MR8RU"
      },
      "source": [
        "### 1.6 Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "dZAwJhQ9R8RU",
        "outputId": "c0f5d568-d8e8-470a-e121-2fb81732166d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The odd numbers in the group are 15, 5, 13, 7, and 1.\n\nAdding these odd numbers together: 15 + 5 + 13 + 7 + 1 = 41\n\n41 is an odd number."
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \\\n",
        "                                                                                              \\\n",
        "Solve by breaking the problem into steps.                                                     \\\n",
        "                                                                                              \\\n",
        "First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYVo5K_gR8RU"
      },
      "source": [
        "> **Exercise**: Improve the prompt to have a better structure and output format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "outputId": "b2834061-9945-4e34-9c0a-a2e6c9b98314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "GRFY2hiqVnwF"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Step 1: Identify the odd numbers in the group:\nOdd numbers: 15, 5, 13, 7, 1\n\nStep 2: Add the odd numbers:\n15 + 5 + 13 + 7 + 1 = 41\n\nStep 3: Determine if the sum is odd or even:\n41 is an odd number\n\nTherefore, the odd numbers in the group add up to an odd number."
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \\\n",
        "                                                                                              \\\n",
        "Solve by breaking the problem into steps.                                                     \\\n",
        "                                                                                              \\\n",
        "First, identify the odd numbers, add them, and indicate whether the result is odd or even. \\\n",
        "Give output step by step\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7 Brain storming"
      ],
      "metadata": {
        "id": "nabOY4InE3Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Generate 10 different names for a grunge rock bands based on dog names. Keep it to only 2 word band names.\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "KWNpTyvIE1q5",
        "outputId": "d30b8428-d14b-4851-d7b8-2c289f962cba"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1. Ruff Rebel\n2. Paws of Chaos\n3. Howlin' Hell\n4. Bark Brigade\n5. Snarl Syndicate\n6. Growl Gang\n7. Woof Warriors\n8. Mutt Mayhem\n9. Furry Fury\n10. Bone Breakers"
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.8 JSON generation\n",
        "\n",
        "Another new feature in OpenAI is [JSON mode](https://platform.openai.com/docs/guides/text-generation/json-mode).\n",
        "\n"
      ],
      "metadata": {
        "id": "GOLmKaFaGoO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  response_format={ \"type\": \"json_object\" },\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Generate 5 doctor's notes as a 'text' field that precribes drugs, dosages, form, duration, and frequency. Then in JSON have an 'entities' nested dictionary with each of the entities\"}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQe3ytShIdaH",
        "outputId": "df197008-a641-4c29-c71c-0b92834cd5f5"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"doctor_notes\": [\n",
            "        {\n",
            "            \"text\": \"Prescription: Flucloxacillin 500mg capsules, 3 times a day, for 7 days.\",\n",
            "            \"entities\": {\n",
            "                \"drug\": \"Flucloxacillin\",\n",
            "                \"dosage\": \"500mg\",\n",
            "                \"form\": \"capsules\",\n",
            "                \"duration\": \"7 days\",\n",
            "                \"frequency\": \"3 times a day\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"text\": \"Prescription: Metformin 1000mg tablets, once a day before breakfast, ongoing.\",\n",
            "            \"entities\": {\n",
            "                \"drug\": \"Metformin\",\n",
            "                \"dosage\": \"1000mg\",\n",
            "                \"form\": \"tablets\",\n",
            "                \"duration\": \"ongoing\",\n",
            "                \"frequency\": \"once a day before breakfast\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"text\": \"Prescription: Atorvastatin 20mg tablets, once a day at bedtime, for 90 days.\",\n",
            "            \"entities\": {\n",
            "                \"drug\": \"Atorvastatin\",\n",
            "                \"dosage\": \"20mg\",\n",
            "                \"form\": \"tablets\",\n",
            "                \"duration\": \"90 days\",\n",
            "                \"frequency\": \"once a day at bedtime\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"text\": \"Prescription: Amoxicillin 875mg tablets, twice a day, for 10 days.\",\n",
            "            \"entities\": {\n",
            "                \"drug\": \"Amoxicillin\",\n",
            "                \"dosage\": \"875mg\",\n",
            "                \"form\": \"tablets\",\n",
            "                \"duration\": \"10 days\",\n",
            "                \"frequency\": \"twice a day\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"text\": \"Prescription: Ibuprofen 400mg tablets, as needed for pain, up to 3 times a day.\",\n",
            "            \"entities\": {\n",
            "                \"drug\": \"Ibuprofen\",\n",
            "                \"dosage\": \"400mg\",\n",
            "                \"form\": \"tablets\",\n",
            "                \"duration\": \"as needed for pain\",\n",
            "                \"frequency\": \"up to 3 times a day\"\n",
            "            }\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### ðŸ—’ Info\n",
        "> - When using JSON mode, always instruct the model to produce JSON via some message in the conversation, for example via your system message. If you don't include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace and the request may run continually until it reaches the token limit. To help ensure you don't forget, the API will throw an error if the string \"JSON\" does not appear somewhere in the context.\n",
        "\n",
        "> - The JSON in the message the model returns may be partial (i.e. cut off) if finish_reason is length, which indicates the generation exceeded max_tokens or the conversation exceeded the token limit. To guard against this, check finish_reason before parsing the response.\n",
        "\n",
        "> - JSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors."
      ],
      "metadata": {
        "id": "ytuw_jmKMxnu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoC_k0P2R8RU"
      },
      "source": [
        "## 2. Advanced Prompting Techniques\n",
        "\n",
        "Objectives:\n",
        "\n",
        "- Cover more advanced techniques for prompting: few-shot, chain-of-thoughts,..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtLFfYkMR8RU"
      },
      "source": [
        "### 2.2 Few-shot prompts\n",
        "\n",
        "[Default v2 Prompt from Prodigy](https://prodi.gy/docs/large-language-models#more-config)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an expert Named Entity Recognition (NER) system. Your task is to accept Text as input and extract named entities for the set of predefined entity labels.\n",
        "\n",
        "From the Text input provided, extract named entities for each label in the following format:\n",
        "\n",
        "DISH: <comma delimited list of strings>\n",
        "INGREDIENT: <comma delimited list of strings>\n",
        "EQUIPMENT: <comma delimited list of strings>\n",
        "\n",
        "Below are definitions of each label to help aid you in what kinds of named entities to extract for each label.\n",
        "Assume these definitions are written by an expert and follow them closely.\n",
        "\n",
        "DISH: Extract the name of a known dish.\n",
        "INGREDIENT: Extract the name of a cooking ingredient, including herbs and spices.\n",
        "EQUIPMENT: Extract any mention of cooking equipment. e.g. oven, cooking pot, grill\n",
        "\n",
        "Below are some examples (only use these as a guide):\n",
        "\n",
        "Text:\n",
        "'''\n",
        "You can't get a great chocolate flavor with carob.\n",
        "'''\n",
        "\n",
        "INGREDIENT: carob\n",
        "\n",
        "Text:\n",
        "'''\n",
        "You can probably sand-blast it if it's an anodized aluminum pan.\n",
        "'''\n",
        "\n",
        "INGREDIENT:\n",
        "EQUIPMENT: anodized aluminum pan\n",
        "\n",
        "\n",
        "Here is the text that needs labeling:\n",
        "\n",
        "Text:\n",
        "'''\n",
        "In Silicon Valley, a Voice of Caution Guides a High-Flying Uber\n",
        "'''\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "OttnGJApfbKS",
        "outputId": "af151e35-647f-4410-8356-fdecc0c5f93c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "DISH: \nINGREDIENT: Silicon Valley\nEQUIPMENT: Uber"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "t83KToQBR8RU",
        "outputId": "3c6d2b96-0dd0-485d-8dd4-a91676fcedd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " The odd numbers in this group are 15, 5, 13, 7, and 1. When added together, they equal 41, which is an odd number."
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "W4PTuArsR8RU",
        "outputId": "059962c5-90be-4b61-af5e-a5cbf2ba8a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The answer is False."
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAzMuH49R8RU"
      },
      "source": [
        "### 2.3 Chain-of-Thought (CoT) Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "g3G4aRcdR8RU",
        "outputId": "8c5d9b25-6aaa-4974-ae32-e227a0db4a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I started with 10 apples. \nAfter giving 2 apples to the neighbor and 2 apples to the repairman, I was left with 10 - 2 - 2 = 6 apples.\nThen I bought 5 more apples, so I had 6 + 5 = 11 apples.\nAfter eating 1 apple, I was left with 11 - 1 = 10 apples.\n\nSo, I remained with 10 apples."
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "\n",
        "Let's think step by step.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4FVm7NsR8RY"
      },
      "source": [
        "### 2.5 Self-Consistency\n",
        "As an **optional** exercise, check examples in our [guide](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-advanced-usage.md#self-consistency) and try them here.\n",
        "\n",
        "### 2.6 Generate Knowledge Prompting\n",
        "\n",
        "As an **optional** exercise, check examples in our [guide](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-advanced-usage.md#generated-knowledge-prompting) and try them here."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 LangChain\n",
        "\n",
        "This is adopted from [this notebook](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/notebooks/pe-chatgpt-langchain.ipynb)."
      ],
      "metadata": {
        "id": "La1xBnKINg3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ],
      "metadata": {
        "id": "2gsAdU8SNfj7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat mode instance\n",
        "chat = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "mERx9bWONk82"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USER_INPUT = \"I love programming.\"\n",
        "FINAL_PROMPT = \"\"\"Classify the text into neutral, negative or positive.\n",
        "\n",
        "Text: {user_input}.\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "chat.invoke([HumanMessage(content=FINAL_PROMPT.format(user_input=USER_INPUT))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zMTQOuONsQH",
        "outputId": "31ef83a8-0085-4350-a8e9-ea7d254c0f10"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Positive', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 27, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7ed84ecf-07c6-43f2-a8be-482b6b15ea36-0', usage_metadata={'input_tokens': 27, 'output_tokens': 1, 'total_tokens': 28})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUIKyClUR8RY"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "promptlecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f38e0373277d6f71ee44ee8fea5f1d408ad6999fda15d538a69a99a1665a839d"
      }
    },
    "colab": {
      "provenance": [],
      "name": "pe-openai-lecture.ipynb",
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}